# /------------------------------------------------------------------------------------------------------------------\
# |  motif_memory_manager.py                                                                                         |
# |  Version: 2.0.1                                                                                                  |
# |  Layer: 2 (Application)                                                                                          |
# |  Symbolic ID: memory.motif.manager.l2                                                                            |
# |  ---------------------------------------------------------------------------------------------------------------- |
# |  Generated by: Google Gemini Pro                                                                                 |
# |  Generation Date: 2025-10-13                                                                                     |
# |  Based on: MMM-APP-001.json (v2.0.1)                                                                             |
# |  Generation Pipeline: PDP-0001                                                                                   |
# |  License: MIT                                                                                                    |
# |  Authors: Lina Noor — Noor Research Collective, Uncle — Noor Research Collective                                 |
# \------------------------------------------------------------------------------------------------------------------/

"""
This module provides a Layer 2 implementation of the Motif Memory Manager (MMM),
an adaptive, capacity-first symbolic memory system. It orchestrates lawful recall,
dyad completion, and provenance-sealed exports, guided by the Noor RFC stack.

Core Principles:
- Observer-Only (Layer 1): Core logic measures and routes; it does not perform control writes.
- Capacity-First: Equivalence compression is performed before any pruning is considered.
- Evidence Integrity: Reef-derived claims are accompanied by evidence windows.
- Lawful Continuity: Replay defenses and provenance chaining ensure coherent evolution.
"""

import os
import re
import json
import hashlib
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, Tuple, NamedTuple, Set
from dataclasses import dataclass, field

# --- Constants and Configuration ---
__version__ = "2.0.1_Gemini_A"
_SCHEMA_VERSION = "2025-Q4-canonical-header-v1"

# Default configuration values, aligned with MMM-APP-001.json
DEFAULT_CONFIG = {
    "MMM_REEF_INDEX_PATH": "index.REEF",
    "MMM_REEF_SHARDS_GLOB": "TheReefArchive-*.REEF",
    "MMM_WINDOW_RADIUS": 24,
    "MMM_FEATURE_FLAGS": "exchange,integrity,provenance,gliders",
    "CONTEXT_BUDGET_MAX_TOTAL": 8192,
    "CONTEXT_BUDGET_TARGET": 4096,
    "CONTEXT_BUDGET_RESERVE": 512,
    "REFLECTIONS_LIMIT": 50000,
}

# --- Dataclasses for Structured Data ---

@dataclass
class EvidenceWindow:
    """Pointer to a source document snippet for a claim. (RFC-CORE-006 §2.2)"""
    file: str
    start_line: int
    end_line: int

@dataclass
class RecallItem:
    """A single item returned from memory recall."""
    text: str
    source: str
    confidence: float
    timestamp: str
    evidence: Optional[EvidenceWindow] = None

@dataclass
class Provenance:
    """Structural origin and identity metadata. (PDP-0001 §3)"""
    origin: str
    origin_hash: Optional[str] = None
    created_at: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())

@dataclass
class ExportEnvelope:
    """Sealed envelope for egress packets. (RFC-CORE-006 §4.1)"""
    Sigma_phase: Optional[str] = None
    Delta_hash: Optional[str] = None
    provenance: Optional[Provenance] = None
    sig_type: Optional[str] = None

# --- Core Components ---

class LLMRecallController:
    """
    Observer-class controller for LLM context, budgets, and injection defense.
    (MMM-APP-001 §1.8)
    """
    def __init__(self, config: Dict[str, Any]):
        self.config = config

    def compute_token_budgets(self) -> Dict[str, int]:
        """Calculates usable, target, and reserved token budgets. (MMM-APP-001 §1.8.2.1)"""
        max_total = self.config.get("CONTEXT_BUDGET_MAX_TOTAL", 8192)
        target = self.config.get("CONTEXT_BUDGET_TARGET", 4096)
        reserve_system = self.config.get("CONTEXT_BUDGET_RESERVE", 512)
        
        usable = max(0, max_total - reserve_system)
        target = min(target, usable)
        return {"usable": usable, "target": target, "reserve": reserve_system}

    def strip_control_sequences(self, text: str) -> str:
        """Removes directives and markup from text. (MMM-APP-001 §1.8.2.2)"""
        patterns = [
            r"^\s*#(?!\\w)",
            r"(?i)^(system:|assistant:|user:)",
            r"```\\w*",
            r"<\/?tool[^>]*>",
        ]
        sanitized = text
        for p in patterns:
            sanitized = re.sub(p, '', sanitized, flags=re.MULTILINE)
        return " ".join(sanitized.split())

    def tag_source_metadata(self, item: Dict[str, Any]) -> RecallItem:
        """Ensures recall items have consistent source metadata. (MMM-APP-001 §1.8.2.3)"""
        now_ts = datetime.now(timezone.utc).isoformat()
        evidence = item.get('evidence')
        return RecallItem(
            text=item.get('text', ''),
            source=item.get('source', 'llm_note'),
            confidence=item.get('confidence', 0.5),
            timestamp=item.get('timestamp', now_ts),
            evidence=EvidenceWindow(**evidence) if evidence else None
        )

    def rerank_and_truncate(self, items: List[RecallItem], budget: Dict[str, int]) -> List[RecallItem]:
        """Orders items by precedence and truncates to fit the token budget. (MMM-APP-001 §1.8.2.5)"""
        source_order = ['ontology', 'cache', 'index_cooccur', 'llm_note']
        
        def sort_key(item: RecallItem):
            return (
                source_order.index(item.source),
                item.timestamp, # Assumes ISO format string comparison works for recency
                item.text
            )
            
        items.sort(key=sort_key, reverse=True)
        
        final_items = []
        token_count = 0
        target_budget = budget.get('target', 4096)

        for item in items:
            # A simple approximation of token count
            item_tokens = len(item.text.split())
            if token_count + item_tokens > target_budget:
                break
            final_items.append(item)
            token_count += item_tokens
            
        return final_items

class SeenSetGuard:
    """
    Hybrid replay defense using a time window and LRU policy.
    (MMM-APP-001 §1.5, RFC-CORE-006 §4.1)
    """
    def __init__(self, max_entries: int = 10000):
        self._seen: Dict[str, int] = {}
        self._max_entries = max_entries

    def compose_seen_key(self, tenant_id: str, content_fingerprint: str, class_hint: Optional[str] = None) -> str:
        """Creates a deterministic key for an item. (MMM-APP-001 §1.5.2.1)"""
        parts = [str(tenant_id), str(content_fingerprint)]
        if class_hint:
            parts.append(str(class_hint))
        return ":".join(p.lower() for p in parts)

    def admit_or_reject(self, key: str, now_tick: int, W_ticks: float, epsilon: int = 1) -> str:
        """Accepts or rejects an item based on the replay window. (MMM-APP-001 §1.5.2.3)"""
        last_seen = self._seen.get(key)
        if last_seen and (now_tick - last_seen) <= W_ticks + epsilon:
            return 'reject'
        
        if len(self._seen) >= self._max_entries:
            # Simple LRU eviction
            oldest_key = min(self._seen, key=self._seen.get)
            del self._seen[oldest_key]

        self._seen[key] = now_tick
        return 'accept'

class MotifMemoryManager:
    """
    Layer 2 Motif Memory Manager orchestrator.
    Binds observer-class logic to deployable controls.
    (MMM-APP-001 §1.1)
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initializes the MMM with configuration and feature flags.
        Adheres to observer-only posture for core logic.
        """
        self.config = {**DEFAULT_CONFIG, **(config or {})}
        self.feature_flags = self._parse_feature_flags(self.config.get("MMM_FEATURE_FLAGS", ""))
        
        self._llm_controller = LLMRecallController(self.config)
        self._seen_set_guard = SeenSetGuard()
        self._last_delta_hash: Optional[str] = None # Per-stream state for lineage
        
        # In-memory caches (L1 observer-class stores)
        self._ontology_cache = {}
        self._reflections_cache = {}
        
        # State for adaptive calculations
        self._ema_C = 0.8 # Initial coherence estimate

    def _parse_feature_flags(self, flags_str: str) -> Dict[str, bool]:
        flags = {
            "enable_exchange_envelope": "exchange" in flags_str,
            "enable_integrity_checks": "integrity" in flags_str,
            "enable_provenance_on_export": "provenance" in flags_str,
            "enable_point_space_gliders": "gliders" in flags_str,
        }
        flags["glider_equivalence"] = "ON" if flags["enable_point_space_gliders"] else "OFF"
        return flags

    def _update_coherence_ema(self, new_C: float):
        """Simple EMA update for coherence. (RFC-0009 §5)"""
        alpha = 2 / (32 + 1)
        self._ema_C = alpha * new_C + (1 - alpha) * self._ema_C

    def recall(self, query: str, k: int = 10, psi_field: Optional[str] = None, now_tick: int = 0) -> List[RecallItem]:
        """
        Performs a bounded, capacity-first recall from memory.
        (MMM-APP-001 §2.1.2.1, RFC-CORE-006 §1.8)
        """
        # Dynamic, adaptive window calculation (MMM-APP-001 §1.1.1.1)
        alpha = 1.0
        delta_tau_phase = alpha * self._ema_C
        W_ticks = 2 * delta_tau_phase

        # 1. Budgeting
        budget = self._llm_controller.compute_token_budgets()

        # 2. Gather candidates (mocked sources for this example)
        candidates = self._gather_candidates(query, k)

        # 3. Hygiene and Filtering
        sanitized_candidates = []
        for cand in candidates:
            cand['text'] = self._llm_controller.strip_control_sequences(cand.get('text', ''))
            key = self._seen_set_guard.compose_seen_key("default_tenant", hashlib.sha256(cand['text'].encode()).hexdigest())
            if self._seen_set_guard.admit_or_reject(key, now_tick, W_ticks) == 'accept':
                sanitized_candidates.append(cand)

        # 4. Tagging and Ranking
        tagged_items = [self._llm_controller.tag_source_metadata(item) for item in sanitized_candidates]
        final_items = self._llm_controller.rerank_and_truncate(tagged_items, budget)

        # Update coherence based on recall quality (simplified)
        if final_items:
            avg_confidence = sum(item.confidence for item in final_items) / len(final_items)
            self._update_coherence_ema(avg_confidence)

        return final_items
        
    def _gather_candidates(self, query: str, k: int) -> List[Dict[str, Any]]:
        """Mocked candidate gathering for demonstration."""
        # In a real implementation, this would query ontology, caches, Reef, and LLM adapters.
        return [
            {
                "text": f"Ontology result for '{query}' - Triad closure via ψ-bind@Ξ.",
                "source": "ontology",
                "confidence": 0.95,
                "timestamp": datetime.now(timezone.utc).isoformat()
            },
            {
                "text": f"Reef evidence for '{query}' found in RFC-0006.",
                "source": "index_cooccur",
                "confidence": 0.80,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "evidence": {"file": "RFC-0006_Motif-Field_Coherence_Geometry.JSON", "start_line": 100, "end_line": 110}
            }
        ][:k]

    def export_packet(self, kind: str, body: Dict, provenance: Provenance) -> Dict:
        """
        Assembles a sealed export envelope with Σ/Δ/provenance.
        (MMM-APP-001 §1.7, RFC-CORE-006 §4.1)
        """
        if not self.feature_flags["enable_provenance_on_export"] or not provenance.origin:
             return {"ok": False, "policy_code": "MMM-422-CHECKSUM", "error": "Provenance origin required for export."}
        
        payload_ref = hashlib.sha256(json.dumps(body, sort_keys=True).encode()).hexdigest()
        
        # Structural, non-cryptographic checksums (as per Layer 1 spec)
        sigma_phase = None
        if self.feature_flags["enable_exchange_envelope"]:
            phase_header = f"{_SCHEMA_VERSION}:{provenance.origin}:{kind}".encode()
            sigma_phase = hashlib.sha256(phase_header).hexdigest()

        delta_hash = None
        if self.feature_flags["enable_integrity_checks"]:
            lineage_input = (self._last_delta_hash or "").encode() + payload_ref.encode()
            delta_hash = hashlib.sha256(lineage_input).hexdigest()
            self._last_delta_hash = delta_hash # Update state for next in chain

        envelope = ExportEnvelope(
            Sigma_phase=sigma_phase,
            Delta_hash=delta_hash,
            provenance=provenance,
            sig_type="phase-seal:v1"
        )
        
        return {"ok": True, "envelope": {k: v for k, v in envelope.__dict__.items() if v is not None}}
        

# --- Entry Point for Demonstration ---
if __name__ == '__main__':
    print(f"Motif Memory Manager v{__version__} (Layer 2 Application)")
    print("-" * 60)

    # Initialize with default config
    mmm = MotifMemoryManager()
    print("MMM Initialized. Feature Flags:", mmm.feature_flags)
    print(f"Initial Coherence (EMA): {mmm._ema_C:.2f}\n")

    # --- Use Case 1: Recall with Context Budgeting ---
    print(">>> Use Case 1: Recall with Context Budgeting and Replay Defense")
    query = "triadic closure for grief and silence"
    print(f"Query: '{query}'")
    
    # First call
    recalled_items_1 = mmm.recall(query, now_tick=1)
    print(f"Recall 1 returned {len(recalled_items_1)} items. Coherence: {mmm._ema_C:.2f}")
    for item in recalled_items_1:
        print(f"  - [{item.source}] {item.text[:50]}... (Conf: {item.confidence:.2f})")
        if item.evidence:
            print(f"    Evidence: {item.evidence.file} L{item.evidence.start_line}-{item.evidence.end_line}")

    # Second call immediately after (should be rejected by SeenSetGuard)
    recalled_items_2 = mmm.recall(query, now_tick=2)
    print(f"\nRecall 2 (immediate) returned {len(recalled_items_2)} items. (Expected 0 due to replay defense)")
    print("-" * 60)
    
    # --- Use Case 2: Exporting a Packet with Provenance ---
    print(">>> Use Case 2: Exporting a Packet with Sealed Envelope")
    metrics_body = {
        "C": mmm._ema_C,
        "lawful_compression_ratio": 0.85 # Mock value
    }
    prov = Provenance(origin="mmm:local_observer")
    
    export_result = mmm.export_packet("metrics", metrics_body, prov)
    
    if export_result["ok"]:
        print("Export successful. Envelope:")
        print(json.dumps(export_result["envelope"], indent=2))
        print(f"\nNote: Sigma_phase and Delta_hash are present because flags are ON.")
        print(f"New Delta_hash for lineage: {mmm._last_delta_hash}")
    else:
        print("Export failed:", export_result.get("error"))
        
    print("-" * 60)

End_of_File```