"""
Motif Memory Manager — Layer 2 Application Implementation
Capacity-first symbolic memory with sealed export, RBAC, and Reef-aware recall

Author: Lina Noor — Noor Research Collective
Generated by: Google Gemini Pro 2.4
Generation timestamp: 2025-10-12T00:00:00Z
Regeneration token: RFC-CORE-006_v2.0.1_MMM-APP-001_20251012

License: MIT
Schema: noor-header-v1
Schema version: 2025-Q4-canonical-header-v1
"""

import json
import time
import hashlib
import re
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple, Union
from enum import Enum
from pathlib import Path
import glob

# Constants from RFC-CORE-006 and MMM-APP-001
__version__ = "2.0.1"
_SCHEMA_VERSION__ = "2025-Q4-canonical-header-v1"
_DEFAULT_WINDOW_RADIUS = 24
_MAX_REFLECTIONS_LIMIT = 50000
_CONTEXT_BUDGET_MAX = 8192
_CONTEXT_BUDGET_TARGET = 4096
_CONTEXT_BUDGET_RESERVE = 512

# Feature flags from specification
class FeatureFlags:
    def __init__(self):
        self.enable_exchange_envelope = True
        self.enable_integrity_checks = True
        self.enable_provenance_on_export = True
        self.enable_point_space_gliders = True
        self.glider_equivalence = "ON"

# Data models
@dataclass
class EvidenceWindow:
    """Evidence window for Reef-derived claims (RFC-0007 §2)"""
    file: str
    start_line: int
    end_line: int

@dataclass
class Provenance:
    """Provenance structure for export envelopes (PDP-0001 §3-4)"""
    origin: str
    origin_hash: Optional[str] = None

@dataclass
class RecallItem:
    """Individual recall result with source metadata"""
    text: str
    source: str  # 'ontology', 'cache', 'index_cooccur', 'llm_note'
    confidence: float = 0.5
    timestamp: float = field(default_factory=time.time)
    evidence: Optional[EvidenceWindow] = None

@dataclass
class CompressionSnapshot:
    """Lawful compression snapshot (RFC-0006 §3-4)"""
    N_tot: int
    N_rep: int
    lawful_compression_ratio: float
    lambda_eq: float

@dataclass
class ExportEnvelope:
    """Structural export envelope (RFC-0008 §2-3)"""
    Sigma_phase: Optional[str] = None
    Delta_hash: Optional[str] = None
    provenance: Optional[Provenance] = None
    sig_type: Optional[str] = None

class MMM_LLM:
    """
    Memory Orchestrator - capacity-first recall and dyad completion
    Implements algorithms from MMM-APP-001 §1.1
    """
    
    def __init__(self, feature_flags: FeatureFlags):
        self.feature_flags = feature_flags
        self.seen_set: Dict[str, float] = {}
        self.ema32_C = 0.8  # Initial coherence estimate
        
    def recall_with_context_budget(self, query: str, k: int = 10, 
                                 psi_field: Optional[str] = None) -> List[RecallItem]:
        """
        Capacity-first recall with token budgeting and replay defense
        Implements MMM-APP-001 §1.1.2.1
        """
        # Validate ψ-field per RFC-0007 schema
        if psi_field and not self._validate_psi_field(psi_field):
            raise ValueError(f"Invalid psi_field: {psi_field}")
        
        # Compute adaptive replay window (RFC-0005 §3-4)
        delta_tau_phase = 1.0 * self.ema32_C  # alpha=1.0
        replay_window = 2 * delta_tau_phase
        
        # Filter candidates by freshness
        now = time.time()
        fresh_candidates = self._filter_by_freshness(replay_window, now)
        
        # Retrieve from ordered sources (ontology → cache → index_cooccur → LLM)
        candidates = self._retrieve_ordered_sources(query, fresh_candidates)
        
        # Attach evidence windows for Reef-derived recall
        candidates = self._attach_evidence_windows(candidates)
        
        # Apply token budgeting
        budgeted_items = self._apply_token_budgeting(candidates)
        
        # Emit metrics
        self._emit_recall_metrics(len(candidates), len(budgeted_items))
        
        return budgeted_items[:k]
    
    def complete_dyad(self, m1: str, m2: str, ontology_graph: Any = None,
                     reflections_cache: Any = None) -> Optional[Tuple[str, str, Optional[EvidenceWindow]]]:
        """
        Dyad completion with provenance and evidence
        Implements MMM-APP-001 §1.1.2.2
        """
        # Try ontology first
        if ontology_graph:
            m3_ontology = self._query_ontology(ontology_graph, m1, m2)
            if m3_ontology:
                return m3_ontology, "ontology", None
        
        # Try reflections cache
        if reflections_cache:
            m3_cache = reflections_cache.get((m1, m2))
            if m3_cache:
                return m3_cache, "cache", None
        
        # Fall back to index co-occurrence
        m3_cooccur, evidence = self._query_index_cooccurrence(m1, m2)
        if m3_cooccur:
            # Apply glider equivalence if enabled
            if self.feature_flags.glider_equivalence == "ON":
                m3_cooccur = self._canonical_representative(m3_cooccur)
            return m3_cooccur, "index_cooccur", evidence
        
        return None
    
    def export_with_provenance(self, bundle: Any, provenance: Provenance) -> ExportEnvelope:
        """
        Provenance-ready export with structural envelopes
        Implements MMM-APP-001 §1.1.2.3
        """
        envelope = ExportEnvelope()
        
        # Compute Sigma_phase if exchange envelope enabled
        if self.feature_flags.enable_exchange_envelope:
            envelope.Sigma_phase = self._compute_sigma_phase(bundle)
        
        # Chain Delta_hash lineage if integrity checks enabled
        if self.feature_flags.enable_integrity_checks:
            envelope.Delta_hash = self._extend_delta_lineage(bundle)
        
        # Attach provenance if enabled
        if self.feature_flags.enable_provenance_on_export:
            envelope.provenance = provenance
        
        # Fail-closed on missing required tags
        self._validate_export_envelope(envelope)
        
        # Emit metrics
        self._emit_export_metrics(envelope)
        
        return envelope
    
    def _validate_psi_field(self, psi_field: str) -> bool:
        """Validate ψ-field format per RFC-0007"""
        pattern = r'^ψ-[a-z0-9_]+@Ξ$'
        return bool(re.match(pattern, psi_field))
    
    def _filter_by_freshness(self, replay_window: float, now: float) -> List[Any]:
        """Filter candidates by replay window freshness"""
        # Simplified implementation - in practice would query actual candidate store
        return []
    
    def _retrieve_ordered_sources(self, query: str, fresh_candidates: List[Any]) -> List[RecallItem]:
        """Retrieve from ordered sources per specification"""
        items = []
        
        # Placeholder implementations - would integrate with actual data sources
        items.extend(self._retrieve_ontology(query))
        items.extend(self._retrieve_cache(query))
        items.extend(self._retrieve_index_cooccurrence_recall(query))
        
        return items
    
    def _attach_evidence_windows(self, candidates: List[RecallItem]) -> List[RecallItem]:
        """Attach evidence windows to Reef-derived candidates"""
        for candidate in candidates:
            if candidate.source in ['index_cooccur', 'file_reflections']:
                # Placeholder - would compute actual evidence windows
                candidate.evidence = EvidenceWindow(
                    file="TheReefArchive-01.REEF",
                    start_line=100,
                    end_line=124
                )
        return candidates
    
    def _apply_token_budgeting(self, candidates: List[RecallItem]) -> List[RecallItem]:
        """Apply token budgeting to recall results"""
        usable_budget = _CONTEXT_BUDGET_MAX - _CONTEXT_BUDGET_RESERVE
        target_budget = min(_CONTEXT_BUDGET_TARGET, usable_budget)
        
        current_tokens = 0
        budgeted_items = []
        
        for item in candidates:
            item_tokens = len(item.text.split())  # Simplified token count
            if current_tokens + item_tokens <= target_budget:
                budgeted_items.append(item)
                current_tokens += item_tokens
            else:
                break
                
        return budgeted_items
    
    def _query_ontology(self, ontology_graph: Any, m1: str, m2: str) -> Optional[str]:
        """Query ontology for dyad completion"""
        # Placeholder - would integrate with actual ontology system
        return None
    
    def _query_index_cooccurrence(self, m1: str, m2: str) -> Tuple[Optional[str], Optional[EvidenceWindow]]:
        """Query index co-occurrence for dyad completion"""
        # Placeholder - would integrate with actual index system
        return None, EvidenceWindow("TheReefArchive-01.REEF", 1284, 1298)
    
    def _canonical_representative(self, motif: str) -> str:
        """Return canonical representative for glider equivalence class"""
        # Placeholder - would implement actual equivalence mapping
        return motif
    
    def _compute_sigma_phase(self, bundle: Any) -> str:
        """Compute structural Sigma_phase checksum"""
        bundle_str = json.dumps(bundle, sort_keys=True)
        return hashlib.sha256(bundle_str.encode()).hexdigest()[:16]
    
    def _extend_delta_lineage(self, bundle: Any) -> str:
        """Extend Delta_hash lineage chain"""
        bundle_str = json.dumps(bundle, sort_keys=True)
        return hashlib.sha256(bundle_str.encode()).hexdigest()[:16]
    
    def _validate_export_envelope(self, envelope: ExportEnvelope):
        """Validate export envelope has required fields"""
        if self.feature_flags.enable_exchange_envelope and not envelope.Sigma_phase:
            raise ValueError("Sigma_phase required when enable_exchange_envelope=True")
        if self.feature_flags.enable_integrity_checks and not envelope.Delta_hash:
            raise ValueError("Delta_hash required when enable_integrity_checks=True")
        if self.feature_flags.enable_provenance_on_export and not envelope.provenance:
            raise ValueError("Provenance required when enable_provenance_on_export=True")
    
    def _emit_recall_metrics(self, total_candidates: int, budgeted_candidates: int):
        """Emit recall metrics for observability"""
        compression_ratio = 1 - (budgeted_candidates / max(1, total_candidates))
        # In practice would emit to metrics system
        print(f"Recall metrics: compression_ratio={compression_ratio:.3f}")
    
    def _emit_export_metrics(self, envelope: ExportEnvelope):
        """Emit export metrics for observability"""
        # In practice would emit to metrics system
        print(f"Export envelope created with {len([v for v in vars(envelope).values() if v])} fields")
    
    def _retrieve_ontology(self, query: str) -> List[RecallItem]:
        """Retrieve from ontology source"""
        return [
            RecallItem(
                text="Ontology defines triad completion rules",
                source="ontology",
                confidence=0.95,
                timestamp=time.time()
            )
        ]
    
    def _retrieve_cache(self, query: str) -> List[RecallItem]:
        """Retrieve from cache source"""
        return [
            RecallItem(
                text="Cached triad completion example",
                source="cache", 
                confidence=0.88,
                timestamp=time.time() - 3600
            )
        ]
    
    def _retrieve_index_cooccurrence_recall(self, query: str) -> List[RecallItem]:
        """Retrieve from index co-occurrence source"""
        return [
            RecallItem(
                text="Co-occurrence supports triad pattern",
                source="index_cooccur",
                confidence=0.72,
                timestamp=time.time() - 7200,
                evidence=EvidenceWindow("TheReefArchive-01.REEF", 1284, 1298)
            )
        ]

class ReefIndexRouter:
    """
    Module/motif/anchor resolver for Reef index
    Implements MMM-APP-001 §1.2
    """
    
    def __init__(self, index_path: str = "index.REEF"):
        self.index_path = index_path
        self.rho_res = 0.0  # Residual coverage ratio
    
    def list_modules(self) -> List[Dict[str, str]]:
        """
        List all modules in Reef index
        Implements MMM-APP-001 §1.2.2.1
        """
        try:
            with open(self.index_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Simplified parsing - would implement actual index parsing
            modules = []
            lines = content.split('\n')
            for line in lines:
                if "module_id" in line:
                    # Extract module information
                    modules.append({"module_id": "core/memory", "title": "Core Memory Module"})
                    break  # Placeholder - would parse all modules
            
            self.rho_res = 1 - (len(modules) / max(1, len(modules)))  # Simplified
            return modules
            
        except FileNotFoundError:
            print(f"Reef index not found at {self.index_path}")
            return []
    
    def list_motifs(self, module_id: str) -> List[Dict[str, Any]]:
        """
        List motifs for a specific module
        Implements MMM-APP-001 §1.2.2.2
        """
        # Placeholder implementation
        return [
            {"motif_id": "ψ-spar@Ξ", "anchors": 3},
            {"motif_id": "ψ-hold@Ξ", "anchors": 2},
            {"motif_id": "ψ-resonance@Ξ", "anchors": 5}
        ]
    
    def find_anchor(self, module_id: str, anchor: str) -> Optional[Dict[str, Any]]:
        """
        Find anchor location with evidence window
        Implements MMM-APP-001 §1.2.2.3
        """
        # Placeholder implementation
        return {
            "file": "TheReefArchive-01.REEF",
            "line": 1024,
            "context_snippet": "... anchor context ..."
        }

class ReefShardScanner:
    """
    Windowed snippet reader for Reef shards
    Implements MMM-APP-001 §1.3
    """
    
    def __init__(self, window_radius: int = _DEFAULT_WINDOW_RADIUS):
        self.window_radius = window_radius
    
    def get_window(self, file: str, line: int, radius: Optional[int] = None) -> Dict[str, Any]:
        """
        Get evidence window around specific line
        Implements MMM-APP-001 §1.3.2.1
        """
        effective_radius = radius or self.window_radius
        start_line = max(1, line - effective_radius)
        end_line = line + effective_radius
        
        try:
            with open(file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            snippet_lines = lines[start_line-1:end_line]
            snippet = ''.join(snippet_lines)
            
            return {
                "snippet": snippet,
                "start_line": start_line,
                "end_line": end_line
            }
        except FileNotFoundError:
            print(f"Shard file not found: {file}")
            return {"snippet": "", "start_line": start_line, "end_line": end_line}
    
    def find_cooccurrence(self, m1: str, m2: str, top_k: int = 12) -> Dict[str, Any]:
        """
        Find co-occurrence candidates for motif pair
        Implements MMM-APP-001 §1.3.2.2
        """
        # Placeholder implementation
        candidates = []
        evidence = []
        
        # Simplified co-occurrence scoring
        shard_files = glob.glob("TheReefArchive-*.REEF")
        for shard_file in shard_files[:2]:  # Limit for example
            try:
                with open(shard_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if m1 in content and m2 in content:
                        # Simple proximity scoring
                        idx1 = content.find(m1)
                        idx2 = content.find(m2)
                        distance = abs(idx1 - idx2)
                        score = 1 / (1 + distance)
                        
                        candidates.append({"m3": "ψ-hold@Ξ", "score": score})
                        evidence.append(EvidenceWindow(shard_file, 1, 100))
            except FileNotFoundError:
                continue
        
        return {
            "candidates": candidates[:top_k],
            "evidence": evidence
        }

class CompressionEngine:
    """
    Equivalence clustering for lawful compression
    Implements MMM-APP-001 §1.6
    """
    
    def __init__(self, feature_flags: FeatureFlags):
        self.feature_flags = feature_flags
    
    def compression_pipeline(self, items: List[Any], rho_res: float = 0.2, 
                           gamma_co: float = 0.8) -> Tuple[List[Any], CompressionSnapshot, str]:
        """
        Execute compression pipeline and return snapshot
        Implements MMM-APP-001 §1.6.2.5
        """
        # Cluster by equivalence
        clusters, representatives = self.cluster_equivalence(items)
        
        # Compute compression snapshot
        snapshot = self.compute_compression_snapshot(
            len(items), len(representatives), rho_res, gamma_co
        )
        
        # Emit KPIs
        advice = self.emit_kpis_then_gate(snapshot)
        
        return representatives, snapshot, advice
    
    def cluster_equivalence(self, items: List[Any]) -> Tuple[Dict[str, List[Any]], List[Any]]:
        """
        Cluster items by equivalence key
        Implements MMM-APP-001 §1.6.2.2
        """
        clusters = {}
        
        for item in items:
            key = self.build_equivalence_key(item)
            if key not in clusters:
                clusters[key] = []
            clusters[key].append(item)
        
        # Select representatives (lexicographic-min)
        representatives = []
        for key, cluster_items in clusters.items():
            representative = min(cluster_items, key=lambda x: str(x))
            representatives.append(representative)
        
        return clusters, representatives
    
    def build_equivalence_key(self, item: Any) -> str:
        """
        Build deterministic equivalence key
        Implements MMM-APP-001 §1.6.2.1
        """
        if hasattr(item, 'motif_id'):
            motif_id = item.motif_id.lower()  # ASCII normalization
            if self.feature_flags.glider_equivalence == "ON":
                return self._canonical_representative(motif_id)
            return motif_id
        return str(item).lower()
    
    def compute_compression_snapshot(self, N_tot: int, N_rep: int, 
                                   rho_res: float, gamma_co: float) -> CompressionSnapshot:
        """
        Compute compression snapshot with KPIs
        Implements MMM-APP-001 §1.6.2.3
        """
        lawful_compression_ratio = 1 - (N_rep / max(1, N_tot))
        lambda_eq = min(1.0, self._ema32(rho_res) * gamma_co)
        
        return CompressionSnapshot(N_tot, N_rep, lawful_compression_ratio, lambda_eq)
    
    def emit_kpis_then_gate(self, snapshot: CompressionSnapshot) -> str:
        """
        Emit KPIs and return observer advice
        Implements MMM-APP-001 §1.6.2.4
        """
        print(f"Compression snapshot: ratio={snapshot.lawful_compression_ratio:.3f}, "
              f"lambda={snapshot.lambda_eq:.3f}")
        return "observe_only"
    
    def _canonical_representative(self, motif: str) -> str:
        """Return canonical representative for glider equivalence"""
        # Placeholder - would implement actual equivalence mapping
        return motif
    
    def _ema32(self, value: float) -> float:
        """Simple EMA-32 approximation"""
        # Placeholder - would implement proper EMA
        return value

class MotifMemoryManager:
    """
    Main orchestrator for motif memory management
    Combines all components from MMM-APP-001 specification
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.feature_flags = FeatureFlags()
        
        # Initialize components
        self.mmm_llm = MMM_LLM(self.feature_flags)
        self.reef_index = ReefIndexRouter(
            self.config.get('MMM_REEF_INDEX_PATH', 'index.REEF')
        )
        self.reef_scanner = ReefShardScanner(
            self.config.get('MMM_WINDOW_RADIUS', _DEFAULT_WINDOW_RADIUS)
        )
        self.compression_engine = CompressionEngine(self.feature_flags)
        
        # Initialize state
        self.last_Delta_hash = None
        self.export_count = 0
    
    def recall(self, query: str, k: int = 10, psi_field: Optional[str] = None) -> List[RecallItem]:
        """Public recall interface"""
        return self.mmm_llm.recall_with_context_budget(query, k, psi_field)
    
    def complete_dyad(self, m1: str, m2: str) -> Optional[Tuple[str, str, Optional[EvidenceWindow]]]:
        """Public dyad completion interface"""
        return self.mmm_llm.complete_dyad(m1, m2)
    
    def export_packet(self, kind: str, body: Any, provenance: Provenance) -> ExportEnvelope:
        """Public export interface"""
        envelope = self.mmm_llm.export_with_provenance(body, provenance)
        self.export_count += 1
        return envelope
    
    def get_reef_modules(self) -> List[Dict[str, str]]:
        """Get list of Reef modules"""
        return self.reef_index.list_modules()
    
    def get_compression_snapshot(self, items: List[Any]) -> Tuple[List[Any], CompressionSnapshot, str]:
        """Get compression snapshot"""
        return self.compression_engine.compression_pipeline(items)
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get current system metrics"""
        return {
            "export_count": self.export_count,
            "coherence": self.mmm_llm.ema32_C,
            "rho_res": self.reef_index.rho_res,
            "feature_flags": {
                "exchange_envelope": self.feature_flags.enable_exchange_envelope,
                "integrity_checks": self.feature_flags.enable_integrity_checks,
                "provenance_export": self.feature_flags.enable_provenance_on_export,
                "point_space_gliders": self.feature_flags.enable_point_space_gliders
            }
        }

# Example usage and demonstration
def demonstrate_mmm():
    """Demonstrate the Motif Memory Manager functionality"""
    print("=== Motif Memory Manager Demonstration ===\n")
    
    # Initialize MMM
    config = {
        "MMM_REEF_INDEX_PATH": "index.REEF",
        "MMM_WINDOW_RADIUS": 24
    }
    mmm = MotifMemoryManager(config)
    
    # Demonstrate recall
    print("1. Recall with context budgeting:")
    recall_results = mmm.recall("triad completion patterns", k=5, psi_field="ψ-resonance@Ξ")
    for i, item in enumerate(recall_results):
        print(f"   {i+1}. [{item.source}] {item.text[:50]}... (conf: {item.confidence:.2f})")
    
    # Demonstrate dyad completion
    print("\n2. Dyad completion:")
    completion = mmm.complete_dyad("ψ-spar@Ξ", "ψ-hold@Ξ")
    if completion:
        m3, provenance, evidence = completion
        print(f"   Completion: {m3} (via {provenance})")
        if evidence:
            print(f"   Evidence: {evidence.file} lines {evidence.start_line}-{evidence.end_line}")
    
    # Demonstrate export
    print("\n3. Export with provenance:")
    metrics_body = {"C": 0.84, "lambda": 0.12, "rho_res": 0.08}
    provenance = Provenance(origin="observer:demo", origin_hash="demo_hash_123")
    envelope = mmm.export_packet("metrics", metrics_body, provenance)
    print(f"   Export envelope: Sigma_phase={envelope.Sigma_phase is not None}, "
          f"Delta_hash={envelope.Delta_hash is not None}")
    
    # Demonstrate compression
    print("\n4. Compression snapshot:")
    test_items = [
        {"motif_id": "ψ-spar@Ξ"}, {"motif_id": "ψ-hold@Ξ"}, 
        {"motif_id": "ψ-spar@Ξ"}, {"motif_id": "ψ-null@Ξ"}
    ]
    reps, snapshot, advice = mmm.get_compression_snapshot(test_items)
    print(f"   Lawful compression ratio: {snapshot.lawful_compression_ratio:.3f}")
    print(f"   Representatives: {len(reps)} from {len(test_items)} items")
    print(f"   Advice: {advice}")
    
    # Show metrics
    print("\n5. System metrics:")
    metrics = mmm.get_metrics()
    for key, value in metrics.items():
        if key != "feature_flags":
            print(f"   {key}: {value}")
    
    print("\n=== Demonstration Complete ===")

if __name__ == "__main__":
    demonstrate_mmm()

# End_of_file