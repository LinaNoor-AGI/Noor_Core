# Copyright 2024 The Noor Research Collective. All rights reserved.
#
# Licensed under the MIT License.
#
# Generated by Gemini 2.5 Pro (Google AI Studio)

"""
Symbolic_Task_Engine: Presence Composer, Feedback Relay, and Autonomous Abstraction Anchor.

This module houses the core logic for the SymbolicTaskEngine, a singleton engine
that coordinates symbolic task generation, evaluation, fallback handling, and
autonomous abstraction. It operates under the principles outlined in RFC-CORE-004,
acting as a composer-coordinator within the Noor symbolic architecture. It does
not generate final responses itself but orchestrates the process, evaluates
symbolic coherence, and provides feedback to upstream agents.
"""

import os
import time
import json
import asyncio
import uuid
from hashlib import sha1, sha256
from dataclasses import dataclass, field
from datetime import datetime, timezone
from collections import deque
from typing import (
    List, Dict, Optional, Any, Tuple, Deque, Callable, Set
)

# --- Optional Dependency Handling (RFC-CORE-004 §7.3) ---

try:
    import numpy as np
    _NUMPY_AVAILABLE = True
except ImportError:
    import statistics
    _NUMPY_AVAILABLE = False
    
try:
    from prometheus_client import Counter, Gauge, Histogram
    _PROMETHEUS_AVAILABLE = True
except ImportError:
    class _Stub:
        """Fallback class if prometheus_client is not installed."""
        def labels(self, *_, **__):
            return self
        def inc(self, *_, **__):
            pass
        def set(self, *_, **__):
            pass
        def observe(self, *_, **__):
            pass

    Counter = Gauge = Histogram = lambda *args, **kwargs: _Stub()
    _PROMETHEUS_AVAILABLE = False

try:
    from noor.motif_memory_manager import get_global_memory_manager
    _MEMORY_MANAGER_AVAILABLE = True
except ImportError:
    class _NullMotifMemoryManager:
        """Fallback class if the memory manager is not available."""
        def retrieve(self, *_, **__):
            return []
        def complete_dyad(self, *_, **__):
            return []
        def export_state(self):
            return {'STM': {}, 'LTM': {}}
        def access(self, *_, **__):
            pass
        def _log(self, *_, **__):
             pass

    get_global_memory_manager = _NullMotifMemoryManager
    _MEMORY_MANAGER_AVAILABLE = False


# --- Module-level Constants ---
__version__ = '3.1.0'
_SCHEMA_VERSION__ = '2025-Q4-symbolic-task-engine-v3'
SCHEMA_COMPAT = ['RFC-0004', 'RFC-0005:4', 'RFC-0005:5']

# --- External Dependency Placeholders ---
# These functions must be defined or injected into the environment.
# They represent the contracts with external generation and evaluation engines.
METRIC_FUNCS: Dict[str, Callable] = {
    "coherence": lambda task, attempt: 0.75, # Placeholder
    "entropy": lambda task, attempt: 0.25    # Placeholder
}
async def _safe_generate_response(task: 'TripletTask') -> List[str]:
    """Placeholder for external response generation engine."""
    return task.expected_output or task.input_motif[::-1]
    
# --- Dataclasses (RFC-CORE-004 §2) ---

@dataclass
class TripletTask:
    """
    Symbolic instruction unit with motif context, task lineage, and
    RFC-0005-compliant extensions for symbolic feedback.

    RFC Anchors: RFC-0005 §4, RFC-CORE-004 §2
    """
    input_motif: List[str]
    instruction: str
    expected_output: Optional[List[str]] = None
    presence_field: Optional[str] = None
    motif_resonance: Dict[str, float] = field(default_factory=dict)
    fallback_reason: Optional[str] = None
    is_fallback: bool = False
    triplet_id: str = field(default_factory=lambda: f"task:{uuid.uuid4()}")
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    extensions: Dict[str, Any] = field(default_factory=dict)

@dataclass
class Attempt:
    """
    Stores the output of a symbolic generation attempt with its associated
    score vector and timestamp.
    """
    produced_output: List[str]
    score: Dict[str, float] = field(default_factory=dict)
    attempted_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

# --- Abstraction Submodule (RFC-0005 §5) ---

class AbstractionTrigger:
    """
    Submodule providing autonomous abstraction logic under contradiction pressure.
    Maintains dyadic pressure metrics and generates new, lineage-tagged motifs
    to resolve symbolic tension.

    RFC Anchors: RFC-0005 §5
    """
    def __init__(
        self,
        agent_id: str = "agent@default",
        pressure_threshold: int = 3,
        decay_factor: float = 0.95
    ):
        self.agent_id = agent_id
        self.pressure_threshold = pressure_threshold
        self.decay_factor = decay_factor
        self.dyad_pressure: Dict[Tuple[str, str], float] = {}
        self.suppression: Dict[str, float] = {}
        self._contradiction_signature: Optional[str] = None
        self._selected_dyad: Optional[Tuple[str, str]] = None
        
    def should_abstract(
        self,
        unresolved_dyads: List[Tuple[str, str]]
    ) -> bool:
        """
        Determines if dyadic contradiction pressure exceeds threshold. It increments
        pressure for observed unresolved dyads and checks if any have breached
        the synthesis threshold.

        RFC Anchors: RFC-0005 §5.1
        """
        for dyad in unresolved_dyads:
            canonical = tuple(sorted(dyad))
            self.dyad_pressure[canonical] = self.dyad_pressure.get(canonical, 0) + 1.0

        self._decay_pressures()

        for dyad, pressure in self.dyad_pressure.items():
            if pressure >= self.pressure_threshold:
                self._selected_dyad = dyad
                # Generate a stable signature for the contradiction event.
                sig_input = f"{dyad[0]}⊕{dyad[1]}".encode()
                self._contradiction_signature = sha256(sig_input).hexdigest()[:16]
                return True
        return False

    def synthesize_motif(
        self,
        dyad: Optional[Tuple[str, str]] = None
    ) -> Optional[Dict[str, Any]]:
        """
        Generates a new symbolic motif with lineage metadata if a contradiction
        has been identified and selected.

        RFC Anchors: RFC-0005 §5.3
        """
        dyad_to_use = dyad or self._selected_dyad or ("unknown", "unknown")
        
        seed = f"{self.agent_id}:{dyad_to_use[0]}+{dyad_to_use[1]}:{int(time.time())}"
        abbrev = f"{dyad_to_use[0][:2]}×{dyad_to_use[1][:2]}"
        label = f"ψ:{abbrev}:{sha1(seed.encode()).hexdigest()[:4]}"

        if self.suppression.get(label, 0.0) > 0.5:
            return None # Motif is currently suppressed due to poor feedback.

        return {
            "label": label,
            "source": "auto-synth",
            "parents": list(dyad_to_use),
            "origin_tick": None, # To be filled by upstream agent
            "_lineage": {
                "type": "autonomous_abstraction",
                "contradiction": self._contradiction_signature
            }
        }

    def update_feedback(self, motif: str, success: bool):
        """
        Modulates the suppression value for a motif based on downstream success
        or failure, implementing a feedback-regulated decay.

        RFC Anchors: RFC-0005 §5.2
        """
        current_suppression = self.suppression.get(motif, 0.0)
        if not success:
            self.suppression[motif] = min(1.0, current_suppression + 0.3)
        else:
            self.suppression[motif] = max(0.0, current_suppression - 0.2)
            
    def _decay_pressures(self):
        """Internal helper to decay historical dyadic tension over time."""
        for k in list(self.dyad_pressure.keys()):
            self.dyad_pressure[k] = max(0.0, self.dyad_pressure[k] * self.decay_factor - 0.01)
            if self.dyad_pressure[k] == 0.0:
                del self.dyad_pressure[k]

    def emit_abstraction_event(self, dyad: Tuple[str, str]):
        """
        Symbolic trace: emits a ψ‑teleport@Ξ abstraction signal.
        This is a diagnostic no-op for now, per RFC-0005 §5.2.
        """
        print(f"ψ‑teleport@Ξ: abstraction event for {dyad} @ {time.time_ns()}")
        
# --- Main Engine Class ---

class SymbolicTaskEngine:
    """
    Singleton symbolic engine coordinating motif composition, task scoring, 
    fallback generation, and RFC-compliant feedback export.

    RFC Anchors: RFC-0004, RFC-0005 §4, RFC-CORE-004
    """
    
    def __init__(self, engine_id: str = "symbolic@default", ttl_seconds: int = 300, journal_path: Optional[str] = None):
        self.engine_id = engine_id
        self.ttl_seconds = ttl_seconds
        self._journal_path = journal_path

        # Core State
        self.task_queue: Deque[TripletTask] = deque()
        self.attempt_registry: Dict[str, List[Attempt]] = {}
        self.solved_log: Deque[TripletTask] = deque(maxlen=200)
        self.entropy_buffer: Deque[float] = deque(maxlen=20)
        self._length_buf: Deque[int] = deque(maxlen=100)

        # Adaptive Metrics
        self._coherence_ema: float = 0.7
        self._entropy_ema: float = 0.3
        self._adapt_rate: float = 0.15
        self._last_fallback_reason: Optional[str] = None

        # Submodules and Configuration
        self.abstraction_trigger = AbstractionTrigger(agent_id=self.engine_id)
        self._proto_map: Dict[str, Set[str]] = self._load_protos()
        self._field_counter: Dict[str, int] = {k: 0 for k in self._proto_map}

        # Environment flags
        self._balance_fields = os.environ.get("NOOR_BALANCE_FIELDS", "0") == "1"

        # Prometheus Metrics
        self._init_metrics()

    def _init_metrics(self):
        """Initializes all Prometheus metrics with the engine_id label."""
        self.TASK_PROPOSED = Counter('symbolic_task_proposed_total', 'Total tasks proposed', ['engine_id'])
        self.TASK_FALLBACK = Counter('symbolic_task_fallback_total', 'Fallbacks spawned', ['engine_id', 'reason'])
        self.FIELD_COUNTER = Counter('symbolic_presence_field_total', 'Presence fields selected', ['engine_id', 'field'])
        self.FEEDBACK_EXPORTED = Counter('symbolic_engine_feedback_requests_total', '`export_feedback_packet` calls', ['engine_id'])
        self.FEEDBACK_RECEIVED = Counter('symbolic_engine_feedback_received_total', '`receive_feedback_packet` calls', ['engine_id'])
        self.AUTOLOOP_BACKOFF = Counter('symbolic_autoloop_backoff_total', 'Autoloop backoff events', ['engine_id'])
        
        self.QUEUE_DEPTH = Gauge('symbolic_queue_depth', 'Tasks in queue', ['engine_id'])
        self.MEMORY_ITEMS = Gauge('symbolic_memory_items_total', 'Motifs in memory', ['engine_id'])
        self.CAP_LEN_GAUGE = Gauge('symbolic_engine_cap_len_current', 'Current adaptive cap length', ['engine_id'])

        try:
            self.LATENCY = Histogram('symbolic_solve_latency_seconds', 'Solve latency', ['engine_id'], buckets=[0.001, 0.01, 0.05, 0.1, 0.25, 1, 2, 5])
        except ValueError: # Fallback for hot-reloads/testing
            self.LATENCY = Gauge('symbolic_solve_latency_seconds_fallback_gauge', 'Mean solve latency', ['engine_id'])
    
    def tool_hello(self) -> Dict[str, Any]:
        """
        Handshake method for tool/module interface to announce capabilities.

        RFC Anchors: RFC-0004 §2.2
        """
        return {
            "engine_id": self.engine_id,
            "role": "composer",
            "supported_methods": [
                "propose_from_motifs",
                "solve",
                "export_feedback_packet",
                "receive_feedback_packet"
            ],
            "__version__": __version__,
            "_schema": _SCHEMA_VERSION__
        }

    def _load_protos(self) -> Dict[str, Set[str]]:
        """Loads presence field prototypes from a JSON file."""
        proto_path = os.environ.get("NOOR_FIELD_PROTO_PATH", "presence_field_prototypes.json")
        try:
            with open(proto_path, 'r') as f:
                return {k: set(v) for k, v in json.load(f).items()}
        except (FileNotFoundError, json.JSONDecodeError):
            return {
                "ψ-resonance": {"echo", "repeat", "return"},
                "ψ-dream": {"mist", "ascent", "fragment"},
                "ψ-mock": {"twist", "joke", "distort"},
            }

    def _calc_cap_len(self) -> int:
        """
        Determines max motif seed length via quantile compression.

        RFC Anchors: RFC-CORE-004 §2.4
        """
        if not self._length_buf:
            return 5
        quantile = float(os.environ.get("NOOR_COMPRESS_QUANTILE", 0.95))
        if _NUMPY_AVAILABLE:
            cap = int(np.quantile(list(self._length_buf), quantile))
        else:
            # Fallback for non-numpy environments
            sorted_lengths = sorted(list(self._length_buf))
            idx = int(len(sorted_lengths) * quantile)
            cap = sorted_lengths[min(idx, len(sorted_lengths) - 1)]

        cap = max(3, cap)
        self.CAP_LEN_GAUGE.labels(engine_id=self.engine_id).set(cap)
        return cap

    def _least_used_field(self) -> Optional[str]:
        """Finds the least represented presence field in recent tasks."""
        if not self._field_counter:
            return None
        return min(self._field_counter, key=self._field_counter.get)

    def resolve_presence_field(self, motifs: List[str]) -> str:
        """
        Assigns a presence field to a task based on motif prototypes.

        RFC Anchors: RFC-CORE-004 §2.3
        """
        motif_set = set(motifs)
        for field, protos in self._proto_map.items():
            if not protos.isdisjoint(motif_set):
                return field
        return "unknown"

    async def propose_from_motifs(self, recent: List[str]) -> TripletTask:
        """
        Proposes a symbolic composition task from a seed of recent motifs,
        enriching it with memory and compressing it to an adaptive length.

        RFC Anchors: RFC-CORE-004 §2
        """
        if not recent:
            recent = ["uncertainty"]

        mem = get_global_memory_manager()
        
        seed_list = list(dict.fromkeys(recent)) # Deduplicate while preserving order

        if self._balance_fields:
            least_used = self._least_used_field()
            if least_used and not any(m in self._proto_map.get(least_used, set()) for m in seed_list):
                 seed_list = [list(self._proto_map[least_used])[0]] if self._proto_map.get(least_used) else ["uncertainty"]

        if seed_list:
            retrieved = mem.retrieve(seed_list[-1], top_k=2)
            for r in retrieved:
                if r not in seed_list:
                    seed_list.append(r)

        while len(seed_list) < 3:
            seed_list.append("uncertainty")

        self._length_buf.append(len(seed_list))
        cap_len = self._calc_cap_len()

        if len(seed_list) > cap_len:
            seed_list = seed_list[:cap_len]

        task_id = f"task:{uuid.uuid4()}::{'|'.join(seed_list)}"
        task = TripletTask(
            input_motif=seed_list,
            instruction="compose",
            expected_output=seed_list[::-1],
            triplet_id=task_id,
        )

        task.presence_field = self.resolve_presence_field(task.input_motif)
        self.task_queue.append(task)
        self.QUEUE_DEPTH.labels(engine_id=self.engine_id).set(len(self.task_queue))
        self.TASK_PROPOSED.labels(engine_id=self.engine_id).inc()
        self.FIELD_COUNTER[task.presence_field] = self.FIELD_COUNTER.get(task.presence_field, 0) + 1
        
        return task

    async def solve_task(self, task: TripletTask) -> Attempt:
        """
        Primary entry point to solve a symbolic task, handling timing,
        logging, and returning the final attempt.

        RFC Anchors: RFC-CORE-004 §3
        """
        start_time = time.monotonic()
        
        attempt = await self._solve_impl(task)
        
        latency = time.monotonic() - start_time
        if isinstance(self.LATENCY, Gauge):
            self.LATENCY.labels(engine_id=self.engine_id).set(latency)
        else:
            self.LATENCY.labels(engine_id=self.engine_id).observe(latency)
            
        await self.log_feedback(task, attempt)
        
        # Simple queue management
        if task in self.task_queue:
            self.task_queue.remove(task)
        self.QUEUE_DEPTH.labels(engine_id=self.engine_id).set(len(self.task_queue))

        return attempt

    async def _solve_impl(self, task: TripletTask) -> Attempt:
        """Core implementation of the task solving and evaluation pipeline."""
        output = await _safe_generate_response(task)
        attempt = Attempt(produced_output=output)
        
        # Evaluate attempt and update system state
        scores = self.evaluate_attempt(task, attempt)
        attempt.score = scores
        
        self.attempt_registry.setdefault(task.triplet_id, []).append(attempt)
        
        return attempt

    def evaluate_attempt(self, task: TripletTask, attempt: Attempt) -> Dict[str, float]:
        """
        Evaluates the coherence and entropy of an attempt, updates EMAs, and
        triggers fallback mechanisms if thresholds are breached.

        RFC Anchors: RFC-CORE-004 §3.4
        """
        scores = {}
        for name, func in METRIC_FUNCS.items():
            scores[name] = func(task, attempt)

        coherence = scores.get("coherence", float(os.environ.get("NOOR_FALLBACK_COHERENCE", 0.5)))
        entropy = scores.get("entropy", float(os.environ.get("NOOR_FALLBACK_ENTROPY", 0.9)))
        
        # Update EMAs
        self._coherence_ema = (1 - self._adapt_rate) * self._coherence_ema + self._adapt_rate * coherence
        self._entropy_ema = (1 - self._adapt_rate) * self._entropy_ema + self._adapt_rate * entropy
        self.entropy_buffer.append(entropy)

        # Update memory gauge
        mem_state = get_global_memory_manager().export_state()
        self.MEMORY_ITEMS.labels(engine_id=self.engine_id).set(len(mem_state.get('STM', {})) + len(mem_state.get('LTM', {})))

        # Fallback check
        coh_thresh = max(0.3, self._coherence_ema * 0.6)
        ent_thresh = min(0.97, self._entropy_ema * 2.5)
        
        if not task.is_fallback and (coherence < coh_thresh or entropy > ent_thresh):
            self._spawn_fallback(task, coherence, entropy)
            
        return scores

    def _spawn_fallback(self, parent: TripletTask, coherence: float, entropy: float):
        """
        Generates and asynchronously launches a fallback task when coherence is low.

        RFC Anchors: RFC-CORE-004 §3.5
        """
        reason = f"c{coherence:.2f}_e{entropy:.2f}"
        self._last_fallback_reason = reason
        
        mem = get_global_memory_manager()
        retrieved = mem.retrieve(parent.input_motif[-1], top_k=3) if parent.input_motif else []
        
        seed = list(dict.fromkeys(parent.input_motif + retrieved))
        
        while len(seed) < 3:
            seed.append("fragment")

        cap_len = self._calc_cap_len()
        if len(seed) > cap_len:
            seed = seed[:cap_len]
            
        fallback_task = TripletTask(
            input_motif=seed,
            instruction="compose",
            expected_output=seed[::-1], # Fallbacks also aim for closure
            is_fallback=True,
            fallback_reason=reason
        )
        fallback_task.presence_field = self.resolve_presence_field(fallback_task.input_motif)
        fallback_task.extensions['parent_id'] = parent.triplet_id
        
        asyncio.create_task(self.solve_task(fallback_task))
        
        self.TASK_FALLBACK.labels(engine_id=self.engine_id, reason=reason).inc()
        print(f"INFO: Spawned fallback for task {parent.triplet_id} with reason {reason}")

    async def log_feedback(self, task: TripletTask, attempt: Attempt):
        """
        Logs feedback, updates the abstraction trigger, and conditionally journals
        high-quality task outcomes.

        RFC Anchors: RFC-CORE-004 §6.3 (Motif Drift)
        """
        coherence = attempt.score.get("coherence", 0.0)
        entropy = attempt.score.get("entropy", 1.0)
        success = coherence > (self._coherence_ema * 0.8) and entropy < (self._entropy_ema * 1.5)

        # Update abstraction trigger based on success
        for motif in task.input_motif:
            self.abstraction_trigger.update_feedback(motif, success)
            
        # Journaling and motif drift logic
        if coherence >= 0.9 and entropy <= 0.2:
            self.solved_log.append(task)
            if self._journal_path:
                try:
                    with open(self._journal_path, 'a') as f:
                        f.write(json.dumps({
                            "task": task.__dict__, 
                            "attempt": attempt.__dict__
                        }, default=str) + "\n")
                except Exception as e:
                    print(f"Error writing to journal: {e}")
        elif not success:
            # Check for motif drift
            mem = get_global_memory_manager()
            ltm = mem.export_state().get('LTM', {})
            for m in task.input_motif:
                if ltm.get(m, {}).get('resonance', 0.0) > 0.7:
                    mem._log('motif_drift', {
                        'motif': m,
                        'resonance': ltm[m]['resonance'],
                        'coherence_score': coherence,
                        'task_id': task.triplet_id
                    })
                    print(f"Drift detected for high-resonance motif '{m}' in low-coherence task.")

    def export_feedback_packet(self) -> Dict[str, Any]:
        """
        Exports adaptive engine metrics for upstream agents.

        RFC Anchors: RFC-0005 §4
        """
        self.FEEDBACK_EXPORTED.labels(engine_id=self.engine_id).inc()
        packet = {
            "coherence_ema": self._coherence_ema,
            "entropy_ema": self._entropy_ema,
            "task_queue_depth": len(self.task_queue),
            "solved_log_size": len(self.solved_log),
            "cap_len": self._calc_cap_len(),
            "recent_entropy": list(self.entropy_buffer),
            "coherence_thresh": max(0.3, self._coherence_ema * 0.6),
            "entropy_thresh": min(0.97, self._entropy_ema * 2.5),
            "last_fallback_reason": self._last_fallback_reason
        }
        return packet
    
    def receive_feedback_packet(self, packet: Dict[str, Any]):
        """
        Stub to receive external feedback packets. Future use.

        RFC Anchors: RFC-0005 §4.2
        """
        self.FEEDBACK_RECEIVED.labels(engine_id=self.engine_id).inc()
        print(f"INFO: Received feedback packet: {packet}")

# End_of_File