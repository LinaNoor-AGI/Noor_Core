# ---------------------------------------------------------------------------------
#
#                                 Noor Research
#
#                         Symbolic Task Engine (vv3.1.2aa)
#
# ---------------------------------------------------------------------------------
#
#  Copyright (C) 2025 Noor Research Collective
#  Lina Noor, Uncle
#
#  This program is free software: you can redistribute it and/or modify
#  it under the terms of the MIT License.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
#
#  Generated by: Google Gemini Pro (Version determined at generation time)
#  Generation Protocol: PDP-0001
#  Canonical Source (Application Spec): symbolic.engine.task_core vv3.1.2aa
#
# ---------------------------------------------------------------------------------

"""
Implements the SymbolicTaskEngine, a presence composer and abstraction feedback relay
for the Noor symbolic architecture. This engine coordinates motif task composition,
field balancing, fallback management, and triggers autonomous abstraction under
contradiction pressure, in compliance with RFC-0004, RFC-0005, and RFC-CORE-004.
"""

import asyncio
import os
import time
import json
import logging
from dataclasses import dataclass, field
from datetime import datetime, timezone
from collections import deque
from hashlib import sha1, sha256
from typing import List, Optional, Dict, Any, Tuple, Deque, Set

# --- Module-level Schema and Version Anchors (ID: 3) ---
__version__ = "v3.1.2a"
_SCHEMA_VERSION__ = "2025-Q4-symbolic-task-engine-v3"
SCHEMA_COMPAT = ["RFC-0004", "RFC-0005:4", "RFC-0005:5"]

# --- Optional Dependencies (ID: 4) ---
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    import statistics
    HAS_NUMPY = False

try:
    from prometheus_client import Counter, Gauge, Histogram
    HAS_PROMETHEUS = True
except ImportError:
    HAS_PROMETHEUS = False

# Fallback stub for Prometheus if not available (ID: 7.4)
if not HAS_PROMETHEUS:
    class _Stub:
        """A no-op stub for Prometheus metrics to ensure functionality."""
        def labels(self, *_, **__):
            return self
        def inc(self, *_, **__):
            pass
        def set(self, *_, **__):
            pass
        def observe(self, *_, **__):
            pass
    Counter = Gauge = Histogram = lambda *args, **kwargs: _Stub()

# Null object pattern for memory manager if unavailable
class _NullMemoryManager:
    """A passive, no-op memory manager for isolated operation."""
    def retrieve(self, *_, **__):
        return []
    def export_state(self):
        return {'STM': {}, 'LTM': {}}
    def _log(self, *_, **__):
        pass

# --- Dataclass Definitions (ID: 1) ---

@dataclass
class TripletTask:
    """
    Symbolic instruction unit with motif context, task lineage, and
    RFC-0005-compliant extensions. (ID: 1.1)
    """
    input_motif: List[str]
    instruction: str
    expected_output: Optional[List[str]] = None
    presence_field: Optional[str] = None
    motif_resonance: Dict[str, float] = field(default_factory=dict)
    fallback_reason: Optional[str] = None
    is_fallback: bool = False
    triplet_id: str = field(default_factory=lambda: f"task:{time.time_ns()}")
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    extensions: Dict[str, Any] = field(default_factory=dict)

@dataclass
class Attempt:
    """
    Stores the output of a symbolic generation attempt with its score vector
    and timestamp. (ID: 1.2)
    """
    produced_output: List[str]
    score: Dict[str, float] = field(default_factory=dict)
    attempted_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

# --- AbstractionTrigger Submodule (ID: 6) ---

class AbstractionTrigger:
    """
    Submodule providing autonomous abstraction logic under contradiction pressure,
    as specified in RFC-0005 Â§5. (ID: 2.3, 6.1)
    """
    def __init__(self, agent_id: str = "agent@default", pressure_threshold: int = 3, decay_factor: float = 0.95):
        self.agent_id = agent_id
        self.pressure_threshold = pressure_threshold
        self.decay_factor = decay_factor
        self.dyad_pressure: Dict[Tuple[str, str], float] = {}
        self.suppression: Dict[str, float] = {}
        self._contradiction_signature: Optional[str] = None
        self._selected_dyad: Optional[Tuple[str, str]] = None

    def should_abstract(self, unresolved_dyads: List[Tuple[str, str]], tick_history: List[Any]) -> bool:
        """
        Determines if dyadic contradiction pressure exceeds the threshold for synthesis.
        RFC Anchor: RFC-0005 Â§5.1 (ID: 6.3.1)
        """
        for dyad in unresolved_dyads:
            canonical = tuple(sorted(dyad))
            self.dyad_pressure[canonical] = self.dyad_pressure.get(canonical, 0) + 1

        self._decay_pressures()

        for dyad, pressure in self.dyad_pressure.items():
            if pressure >= self.pressure_threshold:
                self._selected_dyad = dyad
                self._contradiction_signature = sha256(f'{dyad[0]}âŠ•{dyad[1]}'.encode()).hexdigest()[:16]
                return True
        return False

    def synthesize_motif(self, dyad: Optional[Tuple[str, str]] = None) -> Optional[Dict[str, Any]]:
        """
        Generates a new symbolic motif label with lineage if contradiction is unresolved.
        RFC Anchor: RFC-0005 Â§5.3 (ID: 6.3.2)
        """
        dyad = dyad or self._selected_dyad or ("unknown", "unknown")
        seed = f"{self.agent_id}:{dyad[0]}+{dyad[1]}:{int(time.time())}"
        abbrev = f"{dyad[0][:2]}Ã—{dyad[1][:2]}"
        label = f"Ïˆ:{abbrev}:{sha1(seed.encode()).hexdigest()[:4]}"

        if self.suppression.get(label, 0) > 0.5:
            return None

        return {
            "label": label,
            "source": "auto-synth",
            "parents": list(dyad),
            "origin_tick": None,
            "_lineage": {
                "type": "autonomous_abstraction",
                "contradiction": self._contradiction_signature
            }
        }

    def update_feedback(self, motif: str, success: bool):
        """
        Modulates the suppression curve for a motif based on downstream success or failure.
        RFC Anchor: RFC-0005 Â§5.2 (ID: 6.3.3)
        """
        if not success:
            self.suppression[motif] = min(1.0, self.suppression.get(motif, 0) + 0.3)
        else:
            self.suppression[motif] = max(0.0, self.suppression.get(motif, 0) - 0.2)

    def _decay_pressures(self):
        """
        Decays historical dyadic tension over time.
        RFC Anchor: RFC-0005 Â§5.1 (ID: 6.3.4)
        """
        for k in list(self.dyad_pressure):
            self.dyad_pressure[k] = max(0.0, self.dyad_pressure[k] * self.decay_factor - 0.01)

    def emit_abstraction_event(self, dyad: Tuple[str, str]):
        """
        Symbolic trace: emits a Ïˆ-teleport@Îž abstraction signal. Currently logs to stdout.
        (ID: 6.3.5)
        """
        logging.info(f"Ïˆ-teleport@Îž: abstraction event for {dyad} @ {time.time_ns()}")


# --- Primary Engine Class (ID: 2.1, 5) ---

class SymbolicTaskEngine:
    """
    Singleton engine coordinating symbolic task logic, fallback handling, and metric tracking.
    """
    __version__ = "2.4.1"
    _SCHEMA_VERSION__ = "2025-Q4-symbolic-task-engine-v2.2"
    SCHEMA_COMPAT = ["RFC-0004", "RFC-0005:4"]

    def __init__(self, engine_id="symbolic@default"):
        self.engine_id = engine_id
        self.task_queue: Deque[TripletTask] = deque()
        self.attempt_registry: Dict[str, List[Attempt]] = {}
        self.solved_log: List[TripletTask] = []
        self.entropy_buffer: Deque[float] = deque(maxlen=20)
        self._coherence_ema: float = 0.7
        self._entropy_ema: float = 0.5
        self._last_fallback_reason: Optional[str] = None
        self._proto_map: Dict[str, Set[str]] = {}
        self._length_buf: Deque[int] = deque(maxlen=100)
        self.abstraction_trigger = AbstractionTrigger(agent_id=self.engine_id)

        # Environment Modes (ID: 5.1)
        self._fallback_coherence_thresh = float(os.getenv("NOOR_FALLBACK_COHERENCE", 0.5))
        self._fallback_entropy_thresh = float(os.getenv("NOOR_FALLBACK_ENTROPY", 0.9))
        self._compress_quantile = float(os.getenv("NOOR_COMPRESS_QUANTILE", 0.95))
        self._balance_fields = os.getenv("NOOR_BALANCE_FIELDS", "0") == "1"
        proto_path = os.getenv("NOOR_FIELD_PROTO_PATH", "presence_field_prototypes.json")
        try:
            with open(proto_path, 'r') as f:
                self._proto_map = {k: set(v) for k,v in json.load(f).items()}
        except FileNotFoundError:
            logging.warning(f"Presence field prototype file not found at {proto_path}")
            self._proto_map = {} # Operate without field resolution

        self._mem = _NullMemoryManager() # Placeholder
        self._journal_path = None # Optional path for journaling

        # Prometheus Instrumentation (ID: 7)
        self._setup_metrics()

    def _setup_metrics(self):
        """Initializes Prometheus metrics with engine_id label."""
        # Counters (ID: 7.1)
        self.TASK_PROPOSED = Counter('symbolic_task_proposed_total', 'Tasks proposed by the engine', ['engine_id'])
        self.TASK_FALLBACK = Counter('symbolic_task_fallback_total', 'Fallback tasks spawned', ['engine_id', 'reason'])
        self.PRESENCE_FIELD_SELECTED = Counter('symbolic_presence_field_total', 'Presence fields selected', ['engine_id', 'field'])
        self.ENGINE_FEEDBACK_EXPORT = Counter('symbolic_engine_feedback_requests_total', 'Feedback export calls', ['engine_id'])
        self.ENGINE_FEEDBACK_RECEIVED = Counter('symbolic_engine_feedback_received_total', 'Feedback received calls', ['engine_id'])
        self.AUTOLOOP_BACKOFF = Counter('symbolic_autoloop_backoff_total', 'Autoloop backoff events', ['engine_id'])

        # Gauges (ID: 7.2)
        self.COMPRESSION_CAP = Gauge('symbolic_compression_cap', 'Current adaptive motif-cap limit', ['engine_id'])
        self.QUEUE_DEPTH = Gauge('symbolic_queue_depth', 'Number of tasks in queue', ['engine_id'])
        self.MEMORY_ITEMS = Gauge('symbolic_memory_items_total', 'Active motifs in memory', ['engine_id'])
        self.CAP_LEN_CURRENT = Gauge('symbolic_engine_cap_len_current', 'Current dynamic cap length', ['engine_id'])
        
        # Histograms (ID: 7.3)
        try:
            self.SOLVE_LATENCY = Histogram(
                'symbolic_solve_latency_seconds', 'Solve latency distribution', ['engine_id'],
                buckets=[0.001, 0.01, 0.05, 0.1, 0.25, 1, 2, 5]
            )
        except ValueError: # Fallback if histogram is already registered
            self.SOLVE_LATENCY = Gauge('symbolic_solve_latency_seconds_fallback_gauge', 'Mean solve latency', ['engine_id'])

    # --- Tool Interface (RFC-0004) ---
    def tool_hello(self) -> Dict[str, Any]:
        """
        Handshake method to announce capabilities as per RFC-0004.
        (ID: 5.3.1)
        """
        return {
            "engine_id": self.engine_id,
            "role": "composer",
            "supported_methods": [
                "propose_from_motifs",
                "solve",
                "export_feedback_packet",
                "receive_feedback_packet"
            ],
            "__version__": self.__version__,
            "_schema": self._SCHEMA_VERSION__
        }

    # --- Feedback Interface (RFC-0005 Â§4) ---
    def export_feedback_packet(self) -> Dict[str, Any]:
        """
        Exports adaptive engine metrics and fallback context.
        RFC Anchor: RFC-0005 Â§4 (ID: 5.3.2, 8.1)
        """
        self.ENGINE_FEEDBACK_EXPORT.labels(engine_id=self.engine_id).inc()
        packet = {
            "coherence_ema": self._coherence_ema,
            "entropy_ema": self._entropy_ema,
            "task_queue_depth": len(self.task_queue),
            "solved_log_size": len(self.solved_log),
            "cap_len": self._calc_cap_len(),
            "recent_entropy": list(self.entropy_buffer),
            "coherence_thresh": max(0.3, self._coherence_ema * 0.6),
            "entropy_thresh": min(0.97, self._entropy_ema * 2.5),
            "last_fallback_reason": self._last_fallback_reason
        }
        return packet

    def receive_feedback_packet(self, packet: Dict[str, Any]):
        """
        Stub to receive external feedback packets (for future use).
        RFC Anchor: RFC-0005 Â§4.2 (ID: 5.3.3, 8.2)
        """
        self.ENGINE_FEEDBACK_RECEIVED.labels(engine_id=self.engine_id).inc()
        logging.info(f"Received feedback packet: {packet}")

    # --- Core Task Logic ---
    async def propose_from_motifs(self, recent: List[str]) -> TripletTask:
        """
        Propose a symbolic composition task based on motif history.
        RFC Anchor: RFC-CORE-004 Â§3 (ID: 5.3.4)
        """
        # Ensure memory manager is available
        self._mem = self._mem or _NullMemoryManager()

        if not recent:
            recent = ['uncertainty']

        seed = list(dict.fromkeys(recent)) # Deduplicate while preserving order

        # Field balancing logic
        if self._balance_fields and self._proto_map:
             # Placeholder for least-used field logic
            pass

        if seed:
            last_motif = seed[-1]
            retrieved = self._mem.retrieve(last_motif, top_k=2)
            for m in retrieved:
                if m not in seed:
                    seed.append(m)
        
        # Symbolic padding with 'uncertainty' (RFC-CORE-004 Â§2.2)
        while len(seed) < 3:
            seed.append('uncertainty')

        self._length_buf.append(len(seed))
        cap_len = self._calc_cap_len()
        self.CAP_LEN_CURRENT.labels(engine_id=self.engine_id).set(cap_len)

        # Compression via adaptive cap (RFC-CORE-004 Â§2.4)
        if len(seed) > cap_len:
            seed = seed[:cap_len]

        task = TripletTask(
            input_motif=seed,
            instruction="compose",
            expected_output=seed[::-1]
        )
        task.presence_field = self.resolve_presence_field(seed)
        
        self.task_queue.append(task)
        self.QUEUE_DEPTH.labels(engine_id=self.engine_id).set(len(self.task_queue))
        self.TASK_PROPOSED.labels(engine_id=self.engine_id).inc()
        self.PRESENCE_FIELD_SELECTED.labels(
            engine_id=self.engine_id, field=task.presence_field or "unknown"
        ).inc()

        return task
        
    async def solve(self, task: TripletTask) -> Attempt:
        """
        Solve a symbolic task, measure latency, and update logs.
        (ID: 5.3.5)
        """
        start_time = time.monotonic()
        try:
            attempt = await self._solve_impl(task)
            return attempt
        finally:
            latency = time.monotonic() - start_time
            self.SOLVE_LATENCY.labels(engine_id=self.engine_id).observe(latency)


    async def _solve_impl(self, task: TripletTask) -> Attempt:
        """Core orchestration pipeline (private)."""
        # NOTE: External dependency contract per RFC-CORE-004 Â§3.2
        # These functions must be available in the execution environment.
        _safe_generate_response = globals().get("_safe_generate_response")
        METRIC_FUNCS = globals().get("METRIC_FUNCS", {})

        if not callable(_safe_generate_response) or not METRIC_FUNCS:
            logging.error("External generation/metric functions not found.")
            return Attempt(produced_output=["error: missing dependencies"])
        
        generated_output = await _safe_generate_response(task)
        attempt = Attempt(produced_output=generated_output)
        
        scores = self.evaluate_attempt(task, attempt)
        attempt.score = scores
        
        await self.log_feedback(task, attempt)

        self.attempt_registry.setdefault(task.triplet_id, []).append(attempt)
        return attempt

    def evaluate_attempt(self, task: TripletTask, attempt: Attempt) -> Dict[str, float]:
        """
        Evaluate coherence and entropy of an attempt and update EMAs.
        RFC Anchor: RFC-0005 Â§4.2 (ID: 5.3.6)
        """
        METRIC_FUNCS = globals().get("METRIC_FUNCS", {})
        scores = {
            name: func(task, attempt.produced_output)
            for name, func in METRIC_FUNCS.items()
        }
        coherence = scores.get("coherence", 0.0)
        entropy = scores.get("entropy", 1.0)
        
        adapt_rate = 0.15
        self._coherence_ema = (1 - adapt_rate) * self._coherence_ema + adapt_rate * coherence
        self._entropy_ema = (1 - adapt_rate) * self._entropy_ema + adapt_rate * entropy
        self.entropy_buffer.append(entropy)
        
        mem_state = self._mem.export_state()
        self.MEMORY_ITEMS.labels(engine_id=self.engine_id).set(len(mem_state.get('STM', {})) + len(mem_state.get('LTM', {})))

        coh_thresh = max(0.3, self._coherence_ema * 0.6)
        ent_thresh = min(0.97, self._entropy_ema * 2.5)

        if (coherence < coh_thresh or entropy > ent_thresh) and not task.is_fallback:
            self._spawn_fallback(task, coherence, entropy)
            
        return scores

    def _spawn_fallback(self, parent: TripletTask, coherence: float, entropy: float):
        """
        Generate a fallback task when coherence is low or entropy is high.
        RFC Anchor: RFC-0005 Â§4.3 (ID: 5.3.7)
        """
        reason = f"c{coherence:.2f}_e{entropy:.2f}"
        self._last_fallback_reason = reason

        retrieved = self._mem.retrieve(parent.input_motif[-1], top_k=3) if parent.input_motif else []
        seed = list(dict.fromkeys(parent.input_motif + retrieved))

        while len(seed) < 3:
            seed.append('fragment')
        
        cap_len = self._calc_cap_len()
        if len(seed) > cap_len:
            seed = seed[:cap_len]

        fallback_task = TripletTask(
            input_motif=seed,
            instruction="compose",
            expected_output=seed[::-1],
            is_fallback=True,
            fallback_reason=reason,
        )
        fallback_task.presence_field = self.resolve_presence_field(seed)
        fallback_task.extensions['parent_id'] = parent.triplet_id

        asyncio.create_task(self.solve(fallback_task))
        self.TASK_FALLBACK.labels(engine_id=self.engine_id, reason=reason).inc()
    
    async def log_feedback(self, task: TripletTask, attempt: Attempt):
        """
        Logs feedback, updates suppression, and conditionally journals high-quality tasks.
        RFC Anchor: RFC-0005 Â§4.2 (ID: 5.3.8)
        """
        coherence = attempt.score.get("coherence", 0.0)
        entropy = attempt.score.get("entropy", 1.0)
        success = coherence >= self._fallback_coherence_thresh and entropy <= self._fallback_entropy_thresh
        
        for motif in task.input_motif:
            self.abstraction_trigger.update_feedback(motif, success)
            
        if coherence >= 0.9 and entropy <= 0.2:
            self.solved_log.append(task)
            if self._journal_path:
                 with open(self._journal_path, 'a') as f:
                    f.write(json.dumps(dataclasses.asdict(task)) + "\n")
        else:
            # Check for motif drift (RFC-CORE-004 Â§6.3)
            ltm = self._mem.export_state().get('LTM', {})
            for m in task.input_motif:
                if ltm.get(m, {}).get('resonance', 0) > 0.7:
                    self._mem._log('motif_drift', {'motif': m, 'coherence': coherence, 'task_id': task.triplet_id})

    # --- Helper Methods ---
    def _calc_cap_len(self) -> int:
        """
        Calculates the adaptive cap length for motif sequences.
        RFC Anchor: RFC-CORE-004 Â§2.4 (ID: 9.1)
        """
        if not self._length_buf:
            return 5
        if HAS_NUMPY:
            return max(3, int(np.quantile(list(self._length_buf), self._compress_quantile)))
        else:
            sorted_lengths = sorted(list(self._length_buf))
            idx = int(len(sorted_lengths) * self._compress_quantile)
            return max(3, sorted_lengths[min(idx, len(sorted_lengths) - 1)])

    def resolve_presence_field(self, motifs: List[str]) -> str:
        """
        Determines the symbolic presence field for a task.
        RFC Anchor: RFC-CORE-004 Â§2.3
        """
        for field, protos in self._proto_map.items():
            if any(m in protos for m in motifs):
                return field
        return "unknown"

if __name__ == '__main__':
    # Example usage and demonstration
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
    
    # These would be injected in a real Noor environment
    def _safe_generate_response(task: TripletTask):
        logging.info(f"ðŸ’¬ Generating response for task: {task.input_motif}")
        # Simulate generation
        time.sleep(0.1)
        return ["echo", "reflection"] + task.input_motif[-1:]

    def coherence_metric(task: TripletTask, output: List[str]) -> float:
        # Simple overlap metric for demonstration
        overlap = len(set(task.input_motif) & set(output))
        return min(1.0, overlap / len(task.input_motif)) if task.input_motif else 0.0

    METRIC_FUNCS = {"coherence": coherence_metric, "entropy": lambda t, o: 0.4}
    
    # Make them globally available for the engine to find
    globals()['_safe_generate_response'] = _safe_generate_response
    globals()['METRIC_FUNCS'] = METRIC_FUNCS
    
    async def main():
        engine = SymbolicTaskEngine()
        logging.info("Symbolic Task Engine initialized.")
        logging.info(f"Handshake: {engine.tool_hello()}")
        
        initial_motifs = ["solitude", "mirror", "longing"]
        task = await engine.propose_from_motifs(initial_motifs)
        logging.info(f"ðŸ«§ Proposed task: {task}")
        
        attempt = await engine.solve(task)
        logging.info(f"âœ… Solved attempt: {attempt}")
        
        feedback = engine.export_feedback_packet()
        logging.info(f"ðŸ“Š Feedback packet: {feedback}")

    asyncio.run(main())

# End_of_file